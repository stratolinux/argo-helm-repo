{{ if .Values.tubesync.enabled }}
---
### config VOLUME
# config Replication Secret
{{ if .Values.replication.enabled }}
apiVersion: v1
kind: Secret
type: Opaque             
metadata:
  name: tubesync-bucket-config
  namespace: {{ .Values.tubesync.namespace }}
stringData:
  RESTIC_REPOSITORY: {{ .Values.replication.restic_repository }}/{{ .Values.tubesync.namespace }}-tubesync-config
  RESTIC_PASSWORD: {{ .Values.replication.restic_password }}
  AWS_ACCESS_KEY_ID: {{ .Values.replication.aws_access_key_id }}
  AWS_SECRET_ACCESS_KEY: {{ .Values.replication.aws_secret_access_key }}
{{ end }}
---
# config ReplicationDestination
{{ if .Values.replication.enabled }}
apiVersion: volsync.backube/v1alpha1
kind: ReplicationDestination
metadata:
  name: tubesync-config-replication-destination
  namespace: {{ .Values.tubesync.namespace }}
spec:
  trigger:
    # manual restore
    manual: "seed-volume-from-backup"
  restic:
    # Name of the Secret with the connection information
    repository: tubesync-bucket-config
    capacity: 5Gi
    copyMethod: Snapshot
    storageClassName: ceph-block
    volumeSnapshotClassName: ceph-block
    accessModes:
      - ReadWriteOnce
{{ end }}
---
# config ReplicationSource
{{ if .Values.replication.enabled }}
apiVersion: volsync.backube/v1alpha1
kind: ReplicationSource
metadata:
  name: tubesync-config-replication-source
  namespace: {{ .Values.tubesync.namespace }}
spec:
  # The PVC to be backed up
  sourcePVC: {{ .Values.tubesync.configPVC }}
  trigger:
    # Take a backup per cron schedule
    schedule: {{ .Values.tubesync.replicationSchedule }}
  restic:
    # Prune the repository (repack to free space) every 2 weeks
    pruneIntervalDays: 14
    # Name of the Secret with the connection information
    repository: tubesync-bucket-config
    # Retention policy for backups
    retain:
      hourly: 2
      daily: 2
      weekly: 2
      monthly: 1
      yearly: 0
    # just in case it was locked
    unlock: unlock-me
    # Snap the source volume prior to taking a backup to ensure a
    # point-in-time image.
    copyMethod: Snapshot
    # The VSC to use if the copy method is Snapshot (default if omitted)
    volumeSnapshotClassName: ceph-block
{{ end }}
---
# config PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ .Values.tubesync.configPVC }}
  namespace: {{ .Values.tubesync.namespace }}
spec:
  storageClassName: ceph-block
  accessModes:
    - ReadWriteOnce
{{ if .Values.replication.enabled }}
  dataSourceRef:
    kind: ReplicationDestination
    apiGroup: volsync.backube
    name: tubesync-config-replication-destination
{{ end }}
  resources:
    requests:
      storage: 5Gi
---
# PV for videos share
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: smb.csi.k8s.io
  name: tubesync-videos-share
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  mountOptions:
    - dir_mode=0777
    - file_mode=0777
    - uid=1000
    - gid=1000
    - noperm
    - mfsymlinks
    - config=strict
    - noserverino  # required to prevent data corruption
  csi:
    driver: smb.csi.k8s.io
    readOnly: false
    # volumeHandle format: {smb-server-address}#{sub-dir-name}#{share-name}
    # make sure this value is unique for every share in the cluster
    volumeHandle: {{ .Values.tubesync.videos.share }}##
    volumeAttributes:
      source: //{{ .Values.tubesync.videos.share }}
    nodeStageSecretRef:
      name: {{ .Values.tubesync.videos.secret }}
      namespace: {{ .Values.tubesync.videos.secretNamespace }}
---
### Claim for videos share
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  namespace: {{ .Values.tubesync.namespace }}
  name: tubesync-videos-claim
spec:
  storageClassName: manual
  volumeName: tubesync-videos-share
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
---
# tubesync helm chart application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: tubesync
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    name: in-cluster
    namespace: {{ .Values.tubesync.namespace }}
  project: default
  source:
    repoURL: https://bjw-s.github.io/helm-charts
    targetRevision: "3.1.0"
    chart: app-template
    helm:
      valuesObject:
        controllers:
          tubesync:
            containers:
              app:
                image:
                  repository: {{ .Values.tubesync.image }}
                  tag: {{ .Values.tubesync.version }}
                securityContext:
                  runAsUser: 0
              env:
                - name: TZ
                  value: Americas/New_York
                - name: PUID
                  value: 1000
                - name: GUID
                  value: 1000
        service:
          app:
            controller: tubesync
            ports:
              http:
                port: 4848
        ingress:
          app:
            enabled: {{ .Values.tubesync.ingress.enabled }}
            hosts:
              - host: {{ .Values.tubesync.ingress.host }}
                paths:
                  - path: /
                    pathType: Prefix
                    service: 
                      name: tubesync
                      port: 4848
        persistence:
          config:
            enabled: true
            existingClaim: {{ .Values.tubesync.configPVC }}
            globalMounts:
              - path: /config
          books:
            enabled: true
            existingClaim: tubesync-videos-claim
            globalMounts:
              - path: /downloads
        autoscaling:
          enabled: false
          minReplicas: 1
          maxReplicas: 1
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
{{ end }}